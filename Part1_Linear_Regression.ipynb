{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear versus Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Getting, understanding, and preprocessing the dataset\n",
    "\n",
    "We first import the standard libaries and some libraries that will help us scale the data and perform some \"feature engineering\" by transforming the data into $\\Phi_2({\\bf x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the boston dataset from sklearn\n",
    "# Load dataset to some variable \n",
    "\n",
    "boston_data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of Y: 2\n",
      "The number of fearutes is:  13\n",
      "The features:  ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "Number of examples in our dataset:  506\n",
      "First two rows:  [[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Create X and Y variables - X holding the .data and Y holding .target\n",
    "\n",
    "x = boston_data.data\n",
    "y = boston_data.target\n",
    "\n",
    "# Reshape Y to be a rank 2 matrix using y.reshape()\n",
    "#y=y.reshape(-1,2)\n",
    "#print(\"Rank of Y:\", np.linalg.matrix_rank(y))\n",
    "#y=y.reshape(-1,1) #to maintain (506, 1) shape\n",
    "#print(y.shape)\n",
    "\n",
    "# Observe the number of features and the number of labels\n",
    "num_features = x.shape[1]\n",
    "num_examples = x.shape[0]\n",
    "feature_names = boston_data.feature_names\n",
    "first_two_rows = x[0:2]\n",
    "\n",
    "print(\"The number of fearutes is: \", num_features)\n",
    "# Printing out the features\n",
    "print(\"The features: \", feature_names)\n",
    "# The number of examples\n",
    "print(\"Number of examples in our dataset: \", num_examples)\n",
    "# Observing the first 2 rows of the data\n",
    "print(\"First two rows: \", first_two_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create polynomial feeatures for the dataset to test linear and ridge regression on data with d = 1 and data with d = 2. Feel free to increase the # of degress and see what effect it has on the training and test error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PolynomialFeatures object with degree = 2. Using PolynomialFeatures(degree=2)\n",
    "# Transform X and save it into X_2 using poly.fit_transform(X)\n",
    "# Simply copy Y into Y_2 \n",
    "\n",
    "# I created a function so I am able to call it when predicting a new case\n",
    "\n",
    "def transform_poly(x):   \n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    x2=poly.fit_transform(x)\n",
    "    y2=copy.deepcopy(y)\n",
    "    \n",
    "    return x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 105)\n",
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "# the shape of X_2 and Y_2 - should be (506, 105) and (506, 1) respectively\n",
    "# because I created a function, this is not able to execute\n",
    "# the result is when x2 and y2 are not a part of a function\n",
    "\n",
    "print(x2.shape)\n",
    "print(y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
    "# Return w values\n",
    "# I created one function: for getting the linear coeff, we can pass alpha as 0\n",
    "# and we don't need to check whether it is linear or ridge in the k_fold validation\n",
    "# it is simpler to distingush them when accumulating the results\n",
    "\n",
    "def get_coeff_linear_ridge_normaleq(x_train, y_train, alpha):\n",
    "    if len(y_train.shape)==1:\n",
    "        y_train = y_train[:,np.newaxis]\n",
    "        \n",
    "    m = alpha*np.identity(x_train.shape[1])\n",
    "    n = np.linalg.pinv(np.matmul(np.transpose(x_train), x_train)+m)\n",
    "    \n",
    "    w=np.matmul(np.matmul(n,np.transpose(x_train)), y_train)\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluate_err_ridge function.\n",
    "# Return the train_error and test_error values\n",
    "\n",
    "def evaluate_err(x_train, x_test, y_train, y_test, w): \n",
    "    pred_train = (np.matmul(x_train, w).squeeze())*np.std(y_train) + np.mean(y_train)\n",
    "    pred_test = (np.matmul(x_test, w).squeeze())*np.std(y_train) + np.mean(y_train)\n",
    "    \n",
    "    temp_train = (y_train-pred_train)**2\n",
    "    temp_test = (y_test-pred_test)**2\n",
    "    \n",
    "    size_train = y_train.shape[0]\n",
    "    size_test = y_test.shape[0]\n",
    "    \n",
    "    train_error = np.sum(temp_train)/size_train\n",
    "    test_error = np.sum(temp_test)/size_test\n",
    "    \n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish writting the k_fold_cross_validation function. \n",
    "# Returns the average training error and average test error from the k-fold cross validation\n",
    "# Sklearns K-Folds cross-validator: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "def k_fold_cross_validation(k, x, y, alpha):\n",
    "    \n",
    "    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n",
    "    total_E_val_test = []\n",
    "    total_E_val_train = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Centering the data so we do not need the intercept term (we could have also chose w_0=average y value)\n",
    "        # Scaling the data matrix\n",
    "        scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        \n",
    "        x_train_mean = np.mean(x_train)\n",
    "        x_train_std = np.std(x_train)\n",
    "        x_train_std = np.where(x_train_std == 0, 1, x_train_std)\n",
    "        \n",
    "        x_train = (x_train - x_train_mean)/x_train_std\n",
    "        x_train_temp = np.ones((x_train.shape[0],x.shape[1] + 1))\n",
    "        x_train_temp[:, :x.shape[1]] = x_train\n",
    "        \n",
    "        x_test = (x_test - x_train_mean)/x_train_std\n",
    "        x_test_temp = np.ones((x_test.shape[0], x.shape[1] + 1))\n",
    "        x_test_temp[:, :x.shape[1]]=x_test\n",
    "        \n",
    "        y_train_mean = np.mean(y_train)\n",
    "        y_train_std = np.std(y_train)\n",
    "        y_train_temp = (y_train - y_train_mean)/y_train_std\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Determine the training error and the test error\n",
    "        \n",
    "        w = get_coeff_linear_ridge_normaleq(x_train_temp, y_train_temp, alpha)\n",
    "        \n",
    "        total_E_val_test_temp, total_E_val_train_temp = evaluate_err(x_train_temp, x_test_temp, y_train, y_test, w)\n",
    "        \n",
    "        total_E_val_test.append(total_E_val_test_temp)\n",
    "        total_E_val_train.append(total_E_val_train_temp)\n",
    "        \n",
    " \n",
    "    return  total_E_val_test, total_E_val_train, w\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Testing Err  21.806183575851065\n",
      "Linear Training Err  23.63606860542815\n",
      "Best alpha:  10.0\n",
      "Ridge Testing Err  21.892901157570186\n",
      "Ridge Training Err  21.892901157570186\n"
     ]
    }
   ],
   "source": [
    "# print the error for the both linear regression and ridge regression\n",
    "# the error should include both training error and testing error\n",
    "\n",
    "test_E_val_linear, train_E_val_linear, w = k_fold_cross_validation(10, x, y, 0)\n",
    "\n",
    "#print(len(test_E_val_linear[0]))\n",
    "\n",
    "print(\"Linear Testing Err \", np.sum(test_E_val_linear)/len(test_E_val_linear))\n",
    "print(\"Linear Training Err \", np.sum(train_E_val_linear)/len(train_E_val_linear))\n",
    "\n",
    "alpha_range = np.logspace(1, 7, num=13)\n",
    "fin_alpha = 0\n",
    "min_test_E = float(\"inf\")\n",
    "min_train_E = float(\"inf\")\n",
    "\n",
    "for i in alpha_range:\n",
    "    test_E_val_ridge, train_E_val_ridge, w = k_fold_cross_validation(10, x, y, i)\n",
    "    avg_test_E = np.sum(test_E_val_ridge)/10\n",
    "    avg_train_E = np.sum(test_E_val_ridge)/10\n",
    "    \n",
    "    if min_test_E > avg_test_E:\n",
    "        min_test_E = avg_test_E\n",
    "        min_train_E = avg_train_E\n",
    "        fin_alpha = i\n",
    "        \n",
    "\n",
    "print(\"Best alpha: \", fin_alpha)\n",
    "print(\"Ridge Testing Err \", min_test_E)\n",
    "print(\"Ridge Training Err \", min_train_E)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Testing Err  21.806183575851065\n",
      "Linear Training Err  23.63606860542815\n",
      "Best alpha:  10.0\n",
      "Ridge Testing Err  10.049055874118878\n",
      "Ridge Training Err  10.049055874118878\n"
     ]
    }
   ],
   "source": [
    "# use the model to predict the new test case.\n",
    "\n",
    "x_t = transform_poly(x)\n",
    "x_t = x_t[:,1:]\n",
    "\n",
    "trein_E_val_linear, Test_E_val_linear, w = k_fold_cross_validation(10, x_t, y, 0)\n",
    "\n",
    "print(\"Linear Testing Err \", np.sum(test_E_val_linear)/len(test_E_val_linear))\n",
    "print(\"Linear Training Err \", np.sum(train_E_val_linear)/len(train_E_val_linear))\n",
    "\n",
    "alpha_range = np.logspace(1, 7, num=13)\n",
    "fin_alpha = 0\n",
    "min_test_E = float(\"inf\")\n",
    "min_train_E = float(\"inf\")\n",
    "\n",
    "for i in alpha_range:\n",
    "    test_E_val_ridge, train_E_val_ridge, w = k_fold_cross_validation(10, x_t, y, i)\n",
    "    avg_test_E = np.sum(test_E_val_ridge)/10\n",
    "    avg_train_E = np.sum(test_E_val_ridge)/10\n",
    "    \n",
    "    if min_test_E > avg_test_E:\n",
    "        min_test_E = avg_test_E\n",
    "        min_train_E = avg_train_E\n",
    "        best_alpha = i\n",
    "        best_w = w\n",
    "        \n",
    "\n",
    "print(\"Best alpha: \", best_alpha)\n",
    "print(\"Ridge Testing Err \", min_test_E)\n",
    "print(\"Ridge Training Err \", min_train_E)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:  10\n",
      "Alpha:  10.0\n",
      "w:  [ 1.93291631e-02 -5.29607482e-02  8.44852208e-02  3.87773560e-02\n",
      "  3.24937182e-02  1.93839150e-01  5.94868364e-02 -1.38933143e-01\n",
      "  1.55683231e-01  4.91859206e-02 -6.98996045e-03  3.07602122e-02\n",
      "  2.32052470e-03  6.92541346e-02  1.91388744e-02  2.50098096e-02\n",
      "  3.40009806e-01 -5.69925239e-02 -8.79819469e-02  1.99377459e-02\n",
      " -8.06025507e-02 -1.30195952e-02  4.56698349e-03  1.16098288e-02\n",
      " -6.11411451e-02 -5.42253757e-03  3.23265195e-02 -5.81087633e-02\n",
      " -1.91430078e-02 -5.34896726e-03  7.13576247e-02  9.12165307e-04\n",
      " -1.73166181e-02 -2.08731570e-02  7.11431920e-02  7.26865184e-02\n",
      " -2.91750474e-02 -8.44708535e-02  6.49981833e-02 -3.88996151e-02\n",
      "  3.19892899e-02 -8.73029389e-02  6.76643816e-02 -1.09684224e-01\n",
      "  8.26411714e-02  9.88321892e-02 -2.46958249e-02  8.20147916e-02\n",
      " -1.57930949e-01  3.87773560e-02 -1.93594223e-01 -1.23308305e-01\n",
      "  6.42956527e-02  1.24583395e-01 -1.53006206e-01 -9.28927521e-02\n",
      "  2.91773001e-02  1.63274533e-01 -4.34923293e-02 -3.84192880e-02\n",
      " -9.40377558e-02 -5.38750698e-02 -1.13988429e-01  5.85567478e-03\n",
      " -3.66011429e-02 -7.30555870e-02  4.69817120e-02 -7.46586553e-02\n",
      "  4.83495132e-01 -4.02229684e-02  7.86655258e-04 -1.56982952e-01\n",
      " -2.57031744e-01 -2.15601832e-01  2.07039916e-01 -3.58443799e-01\n",
      "  3.25665466e-02 -4.58768235e-03  7.21145206e-02  1.44530939e-03\n",
      " -4.59707684e-02 -7.02067541e-02 -1.05262883e-01  1.01195071e-01\n",
      "  6.77020076e-02 -1.10921073e-01  1.07927527e-02 -6.49119029e-02\n",
      "  1.53999413e-01 -7.09468849e-03  5.14595502e-02  1.27274066e-01\n",
      "  5.05670653e-02 -1.41025776e-01  1.06466068e-02  7.86039474e-02\n",
      " -8.11249682e-03 -1.32960316e-01  3.12157127e-02 -1.01118791e-02\n",
      "  3.02328198e-02 -9.16917183e-02 -1.27436756e-01  3.76600971e-01\n",
      "  9.25474974e-15]\n"
     ]
    }
   ],
   "source": [
    "# I would choose the closed form ridge regression model with a second degree\n",
    "# polynomial transformation, as it gives the lowes testing error, with almost\n",
    "# no difference with the training error.\n",
    "# Here are the parameters:\n",
    "\n",
    "print(\"K: \", 10)\n",
    "print(\"Alpha: \", best_alpha)\n",
    "print(\"w: \", best_w.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
